## 2025.04.29 Visualize Attention
## 前回の振り返り & 導入
### The Annotated Transformerは難しかった
- 前回は"The Annotated Transformer"という有名な教材に従って、Transformerを実装しながら学ぼうとした。<br>
- 内容が難しすぎた。モデルの話は前提知識が無いと理解不能。<br>
- 70分ぐらいやって全体の5%ぐらいしか進めなかった<br>
- 図やイラストが無く、文章だけで理解するのは難しい<br>
- 聴講者の反応も重かった<br>
という反省を活かして・・・
### 今回は視覚的にわかる・面白い・軽めな内容
「もっと視覚的に」「もっと楽しく」学ぶことを目指します！<br>
テーマ：Attention を可視化して直感的に理解する<br>
### "Attention"って結局何をしてるのか・何者なのかを体感する
具体的には、Attentionという仕組みを、可視化ツール[[1]](http://github.com/jessevig/bertviz)を使って実際に見ながら学んでいく<br>
今日のゴールは、「Attentionってこういう風に働いているんだな」というイメージを持つこと<br>
## そもそも、Attention とは何か
### 概要
ざっくりいうと、入力系列の中で、特に重要な部分に注目する仕組み<br>
例えば、英文を読んでいるときに、「この単語は大事だな」とか、「この言葉に注意しよう」といった意識を持っているイメージ<br>
たとえば"The cat sat on the mat"という文では、"cat"という単語に特に注目する<br>
```table
【単語列】

The      ★cat★      sat       on       the      mat

【矢印列】

  ↓        ⇩          ⇩         ↓         ↓        ⇩

【注目度イメージ】

(弱い)    (中心)   (やや強い)    (弱い)     (弱い)    (やや強い)
```
上記の図は、catを中心(★)として、各単語のAttentionのイメージを表している。<br>
"cat"から "sat" と "mat" にやや強いAttentionが飛んでいるが、他はほとんど無視しているイメージ<br>
上記の図ではcatを中心としているが、実際の計算では、左から右(The→cat→sat→...)に中心をずらして、全単語それぞれが、全単語に対してAttentionを計算する<br>
モデルも同じで、すべての単語を均等に見るのではなく、単語同士の関係を見ながら、どこに注目すべきかを決めている<br>
少し混乱するかもしれないですが、上記のイメージ図は正確に言うと"Self-Attention"のイメージ図です。 Attention と Self Attention の違いについては、後述します。<br>
### 仕組み
モデルの中では、各単語が「特徴ベクトル」という形で表現され、このベクトルを使って、「今注目すべき単語はどれか？」を判断する<br>
具体的には、クエリ（Query） と キー（Key） というベクトルの内積を計算する<br>
つまり、内積が大きい、つまり似ている単語ほど、強く注目する(cos類似度?)<br>
次に、注目度に応じて**バリュー（Value）**ベクトルを重み付けして、情報をまとめる<br>
この一連の流れが、Dot-product Attentionと呼ばれる仕組み<br>
ここまでで、「Attentionは似ている単語に注目するんだな」というイメージが持てればOK<br>
数式で表すと、以下になる<br>
```math
\mathrm{Attention}(Q,K,V)
  = \mathrm{softmax}\!\left(\frac{Q\,K^{\mathsf T}}{\sqrt{d_k}}\right)V
```
## Transformer とは
### 概要
ここまで Attention について学んできました。ここからは、それがどう実際のモデルに使われているのかを見ていきます。<br>
代表的なモデルとして、 Transformer があります。前回紹介してぐだった奴です。<br>
前回も言いましたが、 Transformer は、もともと機械翻訳のために設計されていて、エンコーダ（入力を理解する）とデコーダ（出力を生成する）の２つの部分から成り立っています(多分誰も覚えてないと思うけど)<br>
```illust
英語 → Encoder(文章→ベクトル表現) → Decoder(ベクトル表現→文章) → 日本語
```
この中で、エンコーダ部分だけを取り出したものを Transformer Encoder と呼びます。<br>
エンコーダ・デコーダ構造のものでも、エンコーダのみでも、デコーダのみでも Transformer と呼ぶみたいです。<br>
Transformer Encoderは、入力された文章を読み取り、単語同士の関係性をSelf-Attentionを使って捉え、文章全体の意味をうまく内部に表現する役割を持っています。<br>
### Attention と Self Attention の違いについて
ここで、Self Attention について少し補足しておきます。<br>
Attentionの説明の時に、Attention と Self Attention は別物だと言いました<br>
まず、Attentionというのは、ざっくり言えば「何かが何かに注目する」という仕組みのことです。<br>
たとえば、翻訳タスクでは、「出力する単語を決めるために、入力文のどこを見ればいいか」というふうに、別のものに対して Attention をする場合があります。<br>
これに対して、Self-Attentionとは、自分自身の中で注目する仕組みです。<br>
たとえば、文の中の単語同士が、「私はどの単語と関係が強いかな？」と考えるイメージです。 Attention の所で出てきたイメージ図がまさにこれに当たります<br>
TransformerのEncoderでは、このSelf-Attentionを何回も繰り返すことで、単語同士の関係を深く理解していきます。<br>
### BERT について
BERT というものを知っていますか？<br>
今日の可視化では、BERT というモデルを扱います。<br>
これは、Transformer Encoderだけを積み重ねたもので、たくさんの文を左右両方向から読み取りながら、単語同士の関係を細かく捉えることができる言語モデルです。<br>
つまり、BERTの内部にはたくさんのAttentionが走っていて、どの単語にどれくらい注目しているかが刻々と変わっています。<br>
これを可視化することで、"モデルがどう意味を理解しているか"が見えるようになる、というわけです！<br>
## 可視化ツール"BERTviz"について

## 参考文献
[1] http://github.com/jessevig/bertviz