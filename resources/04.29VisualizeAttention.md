## 2025.04.29 Visualize Attention
## 前回の振り返り & 導入
### The Annotated Transformerは難しかった
- 前回は"The Annotated Transformer"という有名な教材に従って、Transformerを実装しながら学ぼうとした。<br>
- 内容が難しすぎた。モデルの話は前提知識が無いと理解不能。<br>
- 70分ぐらいやって全体の5%ぐらいしか進めなかった<br>
- 図やイラストが無く、文章だけで理解するのは難しい<br>
- 聴講者の反応も重かった<br>
という反省を活かして・・・
### 今回は視覚的にわかる・面白い・軽めな内容
「もっと視覚的に」「もっと楽しく」学ぶことを目指します！<br>
テーマ：Attention を可視化して直感的に理解する<br>
### "Attention"って結局何をしてるのか・何者なのかを体感する
具体的には、Attentionという仕組みを、可視化ツール[[1]](http://github.com/jessevig/bertviz)を使って実際に見ながら学んでいく<br>
今日のゴールは、「Attentionってこういう風に働いているんだな」というイメージを持つこと<br>
## そもそも、Attention とは何か
### 概要
ざっくりいうと、入力系列の中で、特に重要な部分に注目する仕組み<br>
例えば、英文を読んでいるときに、「この単語は大事だな」とか、「この言葉に注意しよう」といった意識を持っているイメージ<br>
たとえば"The cat sat on the mat"という文では、"cat"という単語に特に注目する<br>
```table
【単語列】

The      ★cat★      sat       on       the      mat

【矢印列】

  ↓        ⇩         ⇩         ↓         ↓         ⇩

【注目度イメージ】

(弱い)    (中心)   (やや強い)   (弱い)     (弱い)    (やや強い)
```
上記の図は、catを中心(★)として、各単語のAttentionのイメージを表している。<br>
"cat"から "sat" と "mat" にやや強いAttentionが飛んでいるが、他はほとんど無視しているイメージ<br>
上記の図ではcatを中心としているが、実際の計算では、左から右(The→cat→sat→...)に中心をずらして、全単語それぞれが、全単語に対してAttentionを計算する<br>
モデルも同じで、すべての単語を均等に見るのではなく、単語同士の関係を見ながら、どこに注目すべきかを決めている<br>
### 仕組み
モデルの中では、各単語が「特徴ベクトル」という形で表現され、このベクトルを使って、「今注目すべき単語はどれか？」を判断する<br>
具体的には、クエリ（Query） と キー（Key） というベクトルの内積を計算する<br>
つまり、内積が大きい、つまり似ている単語ほど、強く注目する(cos類似度?)<br>
次に、注目度に応じて**バリュー（Value）**ベクトルを重み付けして、情報をまとめる<br>
この一連の流れが、Dot-product Attentionと呼ばれる仕組み<br>
ここまでで、「Attentionは似ている単語に注目するんだな」というイメージが持てればOK<br>
数式で表すと、以下になる<br>
```math
\mathrm{Attention}(Q,K,V)
  = \mathrm{softmax}\!\left(\frac{Q\,K^{\mathsf T}}{\sqrt{d_k}}\right)V
```
## Transformer とは

## 参考文献
[1] http://github.com/jessevig/bertviz