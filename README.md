# 大崎研自主勉強会<br>オリエンテーション + The Annotated Transformer
## 概要
- 大崎研自主勉強会についての資料です。<br>
- resoucesフォルダに勉強会で用いる資料を掲載しています。適宜参照してください。<br>
- programフォルダには勉強会でコーディング予定のプログラムを置いているので、適宜参照してください。<br>

## 参考文献一覧
[1] Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman(2022). The Annotated Transformer. https://nlp.seas.harvard.edu/annotated-transformer/<br>
[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser(2017). Attention Is All You Need. Advances in Neural Information Processing Systems 30 (NIPS 2017)<br>
[3] Dzmitry Bahdanau, KyungHyun Cho(2014), Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. https://arxiv.org/abs/1409.0473<br>
[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. https://arxiv.org/abs/1512.03385<br>
[5] Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton. Layer Normalization. https://arxiv.org/abs/1607.06450<br>
[6] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. https://jmlr.org/papers/v15/srivastava14a.html
[7] Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le. Massive Exploration of Neural Machine Translation Architectures. https://arxiv.org/abs/1703.03906
[8] Sebastian Raschka. Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs. https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention?utm_source=chatgpt.com
[9] Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan. MoH: Multi-Head Attention as Mixture-of-Head Attention. https://arxiv.org/abs/2410.11842
[10] Rachel Draelos. The Transformer: Attention Is All You Need. https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/?utm_source=chatgpt.com


[11] 森下篤(2024). Visual Studio Code 実践ガイド. 技術評論社<br>
[12] Bill Ludanovic, 鈴木駿, 長尾高弘(2022). 入門 Python3 第二版. O'Reilly Japan<br>
[13] Al Sweigart, 相川愛三(2023). 退屈なことはPythonにやらせよう 第二版. O'Reilly Japan<br>
[14] Al Sweigart, 岡田祐一(2022). きれいなPythonプログラミング. マイナビ<br>

## 更新履歴
2025.04.09 first commit<br>
2025.04.09 update readme(add structure of readme)<br>
2025.04.09 fix format of reference<br>
2025.04.09 add program folder and update readme<br>
2025.04.10 update readme<br>
2025.04.10 refrect the opinion of yoshiteru<br>
2025.04.10 README.md is decomposed into /resouces/orientation.md and /resouces/TheAnnotatedTransformer1.md.<br>
2025.04.10 add orientation.pdf<br>
2025.04.12 refrect the opinion of Dr.shota<br>
2025.04.20 add requirements.txt<br>
2025.04.21 complete encoder section<br>
2025.04.21 complete decoder section<br>
2025.04.22 add attention section<br>
2025.04.22 add encoder/decoder/attention.py<br>

This material benefited from the assistance of ChatGPT.

Kazuma Aoyama(kazuma-a@lsnl.jp), Yoshiteru Taira(yohsiteru@lsnl.jp) and Shota Inoue(shota@lsnl.jp)
